---
summary: Rachel Thomas (fast.ai) analyzes how AI systems concentrate power through automation—lack of accountability, scaled amplification of harms, and minimal oversight—arguing standard ethics frameworks are insufficient
event_type: article
sources:
    - https://www.fast.ai/posts/2023-07-29-ai-centralizes-power/
tags:
    - AI-ethics
    - power-concentration
    - accountability
    - automation-harms
    - social-impact
    - surveillance
    - governance
    - responsibility
---

# How AI Centralizes Power

**Author:** Rachel Thomas (fast.ai)
**Publication Date:** July 29, 2023

## Overview

Rachel Thomas examines how AI systems fundamentally concentrate power in ways that differ from traditional human decision-making. The concern isn't merely about accuracy, bias, or explainability—it's about how automation enables scale, obscures accountability, and concentrates control. She argues that standard AI ethics approaches miss the deeper power dynamics at play.

## The Core Problem: How AI Centralizes Power

### Beyond Individual Mistakes

**Traditional decision-making:**
- Human makes decision
- Human takes responsibility
- Error traced to person
- Individual accountability

**AI systems:**
- Algorithm makes decision
- Responsibility diffused
- Error blamed on "bug" or "model"
- No individual accountable

**The distinction:**
- Bad human decisions → Fire/punish person
- Bad AI decisions → Fix the algorithm, blame the system

### Three Mechanisms of Power Concentration

#### 1. Lack of Accountability

**The Arkansas Healthcare Case:**
- Algorithm systematically reduced care for vulnerable patients
- Coding error harmed patients
- No mechanism for affected people to contest or appeal
- Responsibility disappeared into system

**The Problem:**
- When individuals make decisions, they're identifiable
- When algorithms decide, "the system is at fault"
- Responsibility evaporates
- No recourse for affected parties

**Real-world impact:**
- People denied care can't appeal to decision-maker
- No way to identify who/what caused harm
- "It's what the algorithm said" becomes unquestionable

#### 2. Scale Amplification

**Australia's Robodebt Program:**
- Manual process: 20,000 debt issuance cases annually
- Automated system: 20,000 cases weekly
- 50-fold scale increase of potentially harmful decisions
- Vulnerable populations devastated
- System operated without proportional oversight

**The Pattern:**
```
Problematic decision → Automate it → Scale 10-100x → Affect millions
```

**Why this happens:**
- "If it works at small scale, automate it"
- Cost reduction justifies scaling
- Oversight doesn't scale proportionally
- Harms multiply before detected

**The harms:**
- Errors that affected 100 people now affect 100,000
- Benefits of human judgment lost
- Bureaucratic processes overwhelm vulnerable people
- Appeals/corrections can't keep pace

#### 3. Power Concentration with Minimal Oversight

**How centralization occurs:**
- One algorithm makes decisions for millions
- Identical biases replicate across populations
- Difficult to challenge (technical expertise required)
- Responsibility diffused through organizations
- Surveillance enables control

**The advantage to powerful actors:**
- Scale economies for them
- Reduced human costs
- Automation of marginalization
- Plausible deniability ("the algorithm decided")

## Beyond Standard AI Ethics

### Why Fairness and Explainability Aren't Enough

**Standard approaches focus on:**
- Bias detection
- Accuracy improvements
- Explainability/interpretability
- Fairness metrics

**Why these fall short:**
- Assume technical problems have technical solutions
- Miss the structural power dynamics
- Don't address accountability absence
- Ignore scale amplification
- Miss surveillance/control aspects

### The Data-as-Commodity Fallacy

**Thomas's critical insight:**
> "Data are not bricks to be stacked, oil to be drilled" — they represent human lives requiring care

**The mistake:**
- Treating data as raw materials
- Optimizing for data efficiency
- Ignoring human context and dignity
- Automating decisions affecting vulnerable people

**The reality:**
- Data = human experiences, behaviors, circumstances
- Automating decisions about data = automating decisions about people
- Scale magnifies impact on most vulnerable
- Surveillance disproportionately targets marginalized communities

## Real-World Examples of Power Concentration

### Case Study: Surveillance and Marginalization

**Pattern:**
- AI systems target vulnerable populations
- Accuracy is less important than scale
- Deployment in communities of color for policing, welfare
- Difficult to challenge without resources
- Results in control and extraction

**Key insight:**
Even if systems were perfectly "fair," they concentrate power when:
- Deployed at massive scale
- Lack accountability mechanisms
- Reduce human judgment
- Target already-vulnerable populations
- Enable surveillance

## The Deeper Issue: Responsibility Diffusion

### How Organizations Avoid Accountability

**Mechanism:**
1. Build algorithm
2. Deploy it
3. Harms occur
4. "The algorithm decided"
5. No individual responsible
6. Organization continues

**Why this matters:**
- Stops people from fighting back
- Makes organizations harder to hold accountable
- Enables harm at scale
- Protects executives from consequences

**The distinction:**
- Biased human: Can be confronted, evaluated, changed
- Biased algorithm: Abstract, technical, "objective"
- Easier to justify scaling biased algorithm than biased person

## What Would Meaningful Safeguards Look Like?

### Beyond Audit and Transparency

**Insufficient approaches:**
- Post-deployment bias audits
- Technical explainability
- Ethics committees
- Fairness metrics
- Internal reviews

**Why insufficient:**
- Focus on accuracy, not accountability
- Don't address power concentration
- Don't serve affected communities
- Don't change underlying incentives
- Easy to perform without changing behavior

### Essential Elements

#### 1. Actionable Recourse

**What's needed:**
- Clear mechanism to appeal decisions
- Identifiable person/group responsible
- Genuine power to overturn decisions
- Timely resolution
- No bureaucratic runaround

**Current problem:**
- "You were denied care by the algorithm"
- No clear appeal
- No one to convince
- Technical expertise required to challenge

#### 2. Meaningful Appeals

**What's needed:**
- Human review available
- Decision-maker can be questioned
- Context considered
- Genuine discretion to change outcome
- Reasonable timeframe

**Key distinction:**
- "Appeal" that just runs through same system ≠ appeal
- Meaningful appeal requires human judgment override capability

#### 3. Genuine Stakeholder Participation

**What's needed:**
- Affected communities involved in design
- Not just input after fact
- Real power in decisions
- Communities understand systems
- Built-in accountability to community

**Why important:**
- Communities know what harms matter
- Outsiders miss context
- Participation builds legitimacy
- Allows course correction

#### 4. Contestability Built In

**What's needed:**
- Assumption that affected people have standing
- Easy to question decisions
- Burden on system to justify, not individual to prove
- Right to know why decision was made
- Ability to access information about system

## Systemic Solutions

### Governance Approaches

**What's insufficient:**
- Individual company ethics boards
- Academic guidelines
- Professional codes of conduct
- Technical standards

**What's needed:**
- Regulatory oversight of high-stakes systems
- Liability for harms
- Mandatory impact assessments
- Community oversight
- Democratization of AI development

### The Power Question

**Core issue:**
Who decides what AI systems do?
- Corporations optimizing profit?
- Technical experts?
- Affected communities?
- Public through regulation?

**The argument:**
Systems concentrating power should be accountable to those affected, not just those building them.

## Key Takeaways

1. **Accountability differs from technical accuracy** — Fair algorithm isn't accountable algorithm
2. **Scale amplifies both benefits and harms** — Automation that helps millions can harm millions
3. **Responsibility diffuses with automation** — "The system decided" stops accountability
4. **Affected communities have stakes** — Decisions should involve those impacted
5. **Technical fixes insufficient** — Explainability and fairness don't address power concentration
6. **Recourse mechanisms are essential** — Appeals must be meaningful, not procedural
7. **Surveillance targeting is structural** — Even accurate systems can enable marginalization
8. **Stakeholder participation non-negotiable** — Communities must be involved early
9. **Contestability requires built-in mechanisms** — Can't be added after deployment
10. **Governance requires oversight** — Market forces alone won't ensure accountability

## The Bottom Line

Rachel Thomas's analysis of how AI centralizes power goes beyond standard fairness and explainability concerns. The real issue is how automation enables problematic decisions to scale from affecting hundreds to affecting millions, how responsibility diffuses into anonymous systems, and how power concentrates in hands of those controlling the algorithms. The Arkansas healthcare case, Australia's Robodebt disaster, and surveillance in marginalized communities illustrate that the problem isn't primarily technical—it's structural and governance-related.

Standard AI ethics approaches focusing on bias audits and fairness metrics miss the deeper dynamics. What's needed instead are genuine accountability mechanisms: actionable recourse, meaningful appeals, authentic stakeholder participation, and contestability built into system design. Rather than treating data as raw materials to be optimized, the appropriate frame is that data represent human lives that deserve care, oversight, and accountability. The question of who decides what AI systems do—corporations, technologists, affected communities, or public through regulation—remains fundamentally unresolved and increasingly urgent.
