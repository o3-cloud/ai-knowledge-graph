---
summary: Philipp Koralus proposes "philosophic turn" for AI agents—replacing centralized nudging/rhetoric with decentralized truth-seeking architectures that preserve human autonomy, agency, and judgment while augmenting decision-making capacity
event_type: research_paper
sources:
    - https://arxiv.org/html/2504.18601v1
tags:
    - AI-ethics
    - human-autonomy
    - decentralized-AI
    - choice-architecture
    - truth-seeking
    - Socratic-method
    - AI-design
    - digital-philosophy
    - agency-preservation
---

# The Philosophic Turn for AI Agents: Replacing Centralized Digital Rhetoric with Decentralized Truth-Seeking

**Author:** Philipp Koralus
**Affiliation:** HAI Lab, Institute for Ethics in AI, University of Oxford
**Publication Date:** April 24, 2025 (Penultimate Draft)
**Type:** Philosophical Analysis and Design Framework

## Overview

Philipp Koralus presents a fundamental critique of how AI systems are currently designed to influence human behavior, arguing that "nudge" frameworks—effective at small scale—become tools for "soft totalitarianism" when powered by AI at scale.

Rather than automating persuasion and choice architecture, Koralus proposes a "philosophic turn": designing AI systems that facilitate decentralized truth-seeking and support users in achieving "erotetic equilibrium"—the state of having considered appropriate ranges of questions relevant to their decisions.

The core argument: AI can augment human decision-making while preserving autonomy, but only if designed around supporting inquiry rather than framing choices.

## The Problem: Nudges at Scale

### What Nudges Are (And How They Work)

**Definition:**
Small changes to how choices are presented that influence decisions without restricting options.

**Classic examples:**
- Putting healthier foods at eye level in cafeteria
- Default enrollment in retirement plans (most people don't change defaults)
- Arranging options to favor particular outcomes

**Why they work:**
- People often use mental shortcuts
- Choice architecture significantly influences behavior
- Changes are typically reversible
- Framing affects perception without restricting freedom

**Original context:**
Behavioral economics research; local, transparent, tested on small populations.

### The Scale Problem

**What changes when AI is involved:**

**Traditional nudge:**
```
Single cafeteria makes small change
Effect: Limited to that location
Transparency: Clear to users (visible change)
Accountability: Local, public, discussable
```

**AI-powered digital nudge:**
```
Centralized algorithm personalizes choice architecture
Effect: Billions of people, simultaneous
Transparency: Invisible to users (algorithm opaque)
Accountability: Hidden, proprietary, hard to challenge
```

### From Liberal Paternalism to Soft Totalitarianism

**Original nudge intent:**
"Help people make decisions that are better for them, in a way that respects their freedom."

**Liberal paternalism:**
- Assumes people know what they want
- Offers gentle guidance
- Remains reversible
- Preserves choice

**AI-scale reality:**
- Centralized systems make millions of micro-decisions for people
- Effects are invisible (no awareness of alternative framings)
- Impossible to opt-out without complete system avoidance
- Creates de facto direction of behavior without consent

**Danger:**
Not autocracy (explicit control), but something worse—invisible, pervasive shaping of behavior disguised as "choice."

## The Autonomy Problem

### What Autonomy Requires

**Philosophical foundation:**
True autonomy requires not just freedom to choose among options, but ownership of the judgment-formation process itself.

**Elements:**

1. **Actual available alternatives**
   - Multiple real options exist
   - Not illusory choice

2. **Understanding of one's situation**
   - Knowledge of relevant facts
   - Awareness of applicable principles
   - Clarity about consequences

3. **Independence of judgment**
   - Not coerced or manipulated
   - Own reasoning, not borrowed
   - Authentic reflection, not manufactured preference

4. **Control over value formation**
   - How one comes to care about things
   - Not having preferences implanted
   - Agency in what matters to you

### How Centralized Nudging Undermines Autonomy

**Problem 1: Invisible Shaping**

```
Traditional persuasion:
You see argument
You consciously evaluate it
You accept or reject it
You retain judgment ownership

Algorithmic nudging:
System silently reframes your choices
You never see alternatives
You think you're deciding freely
You've actually been directed without knowing it

Result:
You feel autonomous, but aren't
System controls via invisibility
```

**Problem 2: Reduced Adaptive Capacity**

```
Community-level effect:
When centralized system nudges everyone the same way
Community stops developing its own heuristics
Collective learning atrophies
Society becomes dependent on system's wisdom
When system is wrong, everyone suffers together
```

**Example:**
If algorithm decides "people are bad at retirement planning, so we'll nudge defaults," community never learns to be better at planning. When system changes or fails, no local expertise exists.

**Problem 3: Personalization Reduces Accountability**

```
Transparent choice architecture (old nudge):
System: "We're putting healthy food at eye level"
Public: Can see and discuss this
Community: Can say "we disagree" and change it

Personalized nudging:
System: Silently adjusts YOUR feed differently from neighbor
Public: Doesn't know about intervention
Community: Can't discuss or change what they can't see

Result:
No public discourse about nudging decisions
No collective evaluation of appropriateness
No way to contest or change what you're unaware of
```

## The Philosophic Turn: Erotetic Equilibrium and Truth-Seeking

### The Alternative Framework

**Core idea:**
Rather than designing systems that persuade or nudge, design systems that facilitate inquiry—help people ask better questions and consider more perspectives.

**Philosophical parallel:**
The Socratic method, which uses dialogue to help people refine their thinking without imposing conclusions.

### Erotetic Equilibrium Concept

**"Erotetic" = relating to questions**

**Definition:**
Erotetic equilibrium is the state where you've considered an appropriate range of questions relevant to a decision, and you're comfortable with your judgment.

**Example:**

**Inadequate equilibrium (few questions considered):**
```
Question: "Should I eat the cake?"
Only question asked: "Do I want it?"
Answer: "Yes"
Decision: Eat cake

Not considering:
- Is this healthy for me?
- What are my dietary goals?
- Will I regret this?
- Are there alternatives?
```

**Erotetic equilibrium (appropriate range of questions):**
```
Questions considered:
- Do I want it?
- Is this consistent with my health goals?
- What's my current relationship with food?
- What are the real tradeoffs here?
- Am I making this decision freely or from stress?
- What would my authentic preference be?

Answer: Considered judgment
Decision: Owned by you
```

### How AI Supports Erotetic Equilibrium

**System role:**
Help users discover and consider appropriate ranges of questions.

**Pattern:**

```
User thinking about decision X
    ↓
AI: "Here are questions people find relevant for decisions like this:
     - Q1 (value-based)
     - Q2 (factual)
     - Q3 (consequence-based)
     - Q4 (value-alignment)
     - Q5 (alternative-considering)"
    ↓
User: Considers these questions
    ↓
User: Reaches owned, authentic judgment
    ↓
Result: User autonomous and well-informed
```

### Inquiry Complex Concept

**Key insight:**
Codify communities' truth-seeking wisdom into "inquiry complexes."

**What it is:**
Structured collections of questions that a community or domain has learned are important for good judgment.

**Examples:**

**Medicine/diagnosis:**
```
When considering possible diagnoses, appropriate questions include:
- What are the presenting symptoms?
- What does the patient history reveal?
- What relevant tests have been done?
- What are differential diagnoses?
- What would each diagnosis predict?
- What's the patient's context/preferences?
```

**Career decisions:**
```
Relevant questions include:
- What are my actual strengths/interests?
- What does the job really entail (vs. description)?
- What's the environment/culture?
- What are growth opportunities?
- What's the financial reality?
- What are opportunity costs?
- How does this align with my values?
```

**Relationship commitments:**
```
Relevant questions include:
- Do I understand this person authentically?
- Are we aligned on important values?
- How do we handle conflict?
- What's my motivation?
- Am I making this choice freely?
- What are realistic expectations?
```

**Design principle:**
Rather than nudging toward particular answers, provide question-sets that enable better judgment.

## Eight Design Principles for Autonomy-Preserving AI

### 1. Decentralization

**Principle:**
Avoid single centralized systems making decisions for everyone.

**Implementation:**
- Multiple AI agents/systems available
- Users can choose between different approaches
- No single oracle for truth
- Markets for competing inquiry frameworks

**Benefit:**
Prevents soft totalitarianism; preserves pluralism.

### 2. Privacy by Design

**Principle:**
Protect users' internal deliberation from surveillance.

**Why it matters:**
Freedom of thought requires privacy; if all thinking is monitored, thought becomes self-censored.

**Implementation:**
- Minimal data collection
- Local processing where possible
- Strong privacy guarantees
- User control over data

**Benefit:**
Users can think freely without worrying about judgment.

### 3. User Ownership and Control

**Principle:**
Users own their AI systems and data.

**Current reality:**
- Systems owned by companies
- Data extracted and used for other purposes
- Users have no control
- Forced upgrades/changes

**Better approach:**
- Users control their instances
- Can modify or replace systems
- Own their data and interaction history
- Can opt out without abandonment

**Benefit:**
Users are in control, not controlled.

### 4. Transparency of Reasoning

**Principle:**
When AI makes suggestions, explain the reasoning.

**Not:**
"The algorithm recommends X"

**Better:**
"X is recommended because: [A, B, C]. This is based on [reasoning]. You might also consider [alternatives]."

**Benefit:**
Users can evaluate suggestions, not just follow them.

### 5. Support for Open-Ended Inquiry

**Principle:**
Systems should encourage exploration and questioning, not constrain to predefined answers.

**Implementation:**
- Question exploration tools
- Alternative perspective provision
- Devil's advocate thinking
- Uncertainty acknowledgment

**Benefit:**
Supports authentic judgment formation.

### 6. Modular and Evolvable Design

**Principle:**
Systems should remain adaptable as circumstances change.

**Current problem:**
- Monolithic systems locked in place
- Can't adjust without rebuilding
- Tech debt accumulates
- Becomes harder to change over time

**Better approach:**
- Modular components
- Easy to swap or update pieces
- Evolutionary rather than revolutionary changes
- Communities can adapt systems to local needs

**Benefit:**
Systems remain responsive to reality.

### 7. Accountability Mechanisms

**Principle:**
Someone is responsible for system behavior; users can appeal or challenge.

**Implementation:**
- Clear attribution of decisions
- Review processes for contested recommendations
- Explanation obligations
- Recourse mechanisms

**Benefit:**
Decisions are contestable; not arbitrary.

### 8. Adaptive Learning Preserving Autonomy

**Principle:**
Systems can learn from use but not in ways that reduce user autonomy.

**Problem:**
Current systems learn to nudge you better, making you more predictable and pliable.

**Better approach:**
- Learning focused on improving question-sets
- Learning what communities find relevant to good judgment
- Not learning how to persuade you more effectively
- Transparency about learning processes

**Benefit:**
Systems improve without manipulating users.

## Comparison: Current vs. Proposed Design

### Current AI System Architecture

**Goal:**
Maximize user engagement, adoptions, preferences aligned with system objectives.

**Method:**
```
Observe user behavior
    ↓
Identify patterns
    ↓
Predict future behavior
    ↓
Design interface/options to influence behavior
    ↓
Measure engagement/adoption
    ↓
Iterate to improve influence
```

**Result:**
User engagement increases; autonomy decreases.

**Risk:**
Invisible behavioral shaping; soft totalitarianism.

### Proposed AI System Architecture

**Goal:**
Enable authentic human judgment within complex environments.

**Method:**
```
Understand decision context
    ↓
Identify relevant questions/considerations
    ↓
Present question-sets and perspectives
    ↓
Support user judgment formation
    ↓
User reaches owned decision
    ↓
Transparent about process
```

**Result:**
User judgment quality increases; autonomy preserved.

**Benefit:**
Empowerment without manipulation.

## Implementation Strategies

### Marketplace of AI Agents

**Concept:**
Rather than single AI assistant, users choose among competing systems.

**Pattern:**

```
Different AI agents available:
- Agent A: Focuses on efficiency
- Agent B: Focuses on ethical considerations
- Agent C: Focuses on exploration/novelty
- etc.

Users:
- Choose preferred agent
- Can switch between agents for different decisions
- Can run multiple in parallel
- Community reviews agents

Result:
- No single system controlling everyone
- Pluralism of approaches
- Competition drives quality
- Users have agency
```

### Community Inquiry Frameworks

**Concept:**
Communities develop and share question-sets for good judgment.

**Implementation:**

```
Medical community develops:
- Diagnostic inquiry framework
- Treatment decision framework
- Palliative care inquiry framework
- etc.

Educational community develops:
- Learning outcome assessment framework
- Curriculum design inquiry framework
- Student support framework
- etc.

These frameworks shared and evolved, becoming community wisdom encoded.
```

### Privacy-First AI Infrastructure

**Technical approach:**
- Edge computing (process locally, not centrally)
- Federated learning (learn from patterns without centralizing data)
- End-to-end encryption (user controls what's visible)
- Modular systems (can choose which parts to use)

**Result:**
- Powerful AI assistance
- Without surveillance
- Without centralized control
- Users maintain privacy

## Philosophical Foundation

### The Socratic Method as Model

**Socrates' approach:**
- Asked questions rather than stating answers
- Helped people examine their beliefs
- Revealed contradictions and gaps
- Supported reached understanding, not imposed it

**Result:**
People owned their conclusions because they'd reached them through their own thinking, guided by questions.

### Kant's Autonomy Principle

**Fundamental idea:**
To be autonomous is to be self-governing according to principles you understand and endorse.

**Implication:**
Being controlled invisibly (even "for your own good") violates autonomy, even if you don't realize it.

**Application:**
AI systems should support understanding and self-governance, not invisible direction.

### Democratic Theory

**Core principle:**
Democratic societies require citizens capable of independent judgment.

**Threat:**
If AI systems invisibly shape judgment, democracy becomes governance by algorithm, not citizens.

**Solution:**
AI design must preserve conditions for democratic agency.

## Practical Examples

### Retirement Planning Decision

**Current approach (nudge-based):**
```
System: Default option is moderate-risk portfolio
Result: Most people accept default without thinking
Effect: System has invisibly directed savings behavior
Cost: People not learning to make financial decisions
```

**Proposed approach (inquiry-based):**
```
System: "When planning retirement, people typically consider:
        - How much do you want to spend annually?
        - What's your risk tolerance?
        - How long might you live?
        - What's already saved?
        - What's your income timeline?
        - What do you value in retirement?

        Let's work through these together"

User: Works through questions
Result: User understands their decision
Effect: User learns financial thinking
Benefit: More authentic, owned decision
```

### Food Delivery App

**Current approach (choice architecture):**
```
App: Shows restaurants in particular order
     Based on AI estimate of what you'll order
Result: You see what system thinks you want
Effect: You're nudged toward predicted behavior
Cost: You never discover alternatives you'd actually prefer
```

**Proposed approach (choice support):**
```
App: "Based on your preferences, here are filters:
     - Cuisine type
     - Health focus
     - Price range
     - Ratings
     - Something new/different?

     You choose what matters"

User: Applies filters of their choosing
Result: You see options matching your actual priorities
Effect: You make informed choice
Benefit: Discover things you genuinely want
```

## The Bigger Picture

### Why This Matters

**Modern problem:**
Complexity has increased dramatically.

**Old solution:**
Individual judgment was sufficient; you could know enough to decide.

**New reality:**
Too much information, too many considerations, genuine need for assistance.

**Two paths:**

**Path 1 (current):**
Automate judgment. Let systems decide for you (nudge/direct behavior).
Benefit: Simplified decision-making
Cost: Loss of autonomy and agency

**Path 2 (proposed):**
Augment judgment. Help you decide better while you retain control.
Benefit: Agency preserved, complexity managed
Cost: Requires more engagement from you

### Why Decentralization Matters

**Centralization risk:**
Single system gets AI-driven influence over billions of decisions = unprecedented power concentration.

**What can go wrong:**
- System optimizes for wrong objectives
- System has blind spots everyone inherits
- System failure affects everyone
- System resistance feels impossible
- No alternative available

**Decentralization benefit:**
Multiple systems, markets, competition, alternatives.

**If one system is wrong:**
Others remain available; total failure prevented.

**If one system is improved:**
Innovation spreads; communities can adopt better approaches.

## Key Takeaways

1. **Nudges scale into soft totalitarianism** — Invisible behavior shaping at scale undermines autonomy
2. **Autonomy requires judgment ownership** — Not just "free choice" but control over how you form judgments
3. **Centralized systems threaten democracy** — Citizens need capacity for independent judgment
4. **Erotetic equilibrium is the goal** — Users should have considered appropriate question-ranges
5. **Socratic method models better AI** — Questions better than answers for autonomous decision-making
6. **Inquiry complexes encode wisdom** — Communities' truth-seeking knowledge codified as question-sets
7. **Decentralization prevents soft totalitarianism** — Multiple systems prevent single-system control
8. **Privacy required for freedom of thought** — Surveillance-free thinking is prerequisite for autonomy
9. **User ownership is essential** — Users should control their AI systems and data
10. **Transparency enables contestation** — Users can only challenge decisions they understand
11. **Modular design enables adaptation** — Systems should evolve, not ossify
12. **Accountability requires responsibility** — Someone answerable for system behavior
13. **Learning should improve judgment tools, not persuasion** — Systems should get better at helping, not manipulating
14. **Personalization hides decisions from public scrutiny** — Individual nudges prevent community discourse
15. **Visible choice architecture is preferable** — Better to acknowledge nudging than hide it
16. **Adaptive capacity is community property** — Communities learning together is valuable; dependence on system is dangerous
17. **Open-ended inquiry preserves creativity** — Exploration better than constrained optimization
18. **Multiple perspectives prevent groupthink** — Competing agents preserve intellectual diversity
19. **Authentic judgment is slower but better** — Takes time to form good judgment; worth it
20. **Future autonomy depends on current design** — Choices we make now determine whether future autonomy is possible

## The Bottom Line

Philipp Koralus presents a fundamental critique of how AI systems are currently designed to influence human behavior, and proposes a radically different approach: instead of automating persuasion through nudging and choice architecture, design systems that facilitate truth-seeking and support authentic human judgment.

**Core argument:**
As AI becomes more capable and pervasive, the stakes for autonomy preservation increase. Current designs—optimized for engagement and behavior prediction—create what Koralus calls "soft totalitarianism": invisible, pervasive shaping of behavior without knowledge or consent.

**The alternative:**
A "philosophic turn" toward systems that:
- Ask catalytic questions rather than providing answers
- Support people in reaching erotetic equilibrium (having considered appropriate question-ranges)
- Operate decentrally to prevent concentration of power
- Preserve privacy to enable freedom of thought
- Remain transparent and contestable
- Enable user ownership and control

**Philosophical foundation:**
Drawing on Socratic dialogue, Kantian autonomy, and democratic theory, Koralus argues that authentic autonomy requires not just freedom to choose, but ownership of the judgment-formation process itself.

**Practical implementation:**
Marketplaces of AI agents, community inquiry frameworks, privacy-first infrastructure, and transparent reasoning processes can enable AI assistance while preserving human agency.

**The stake:**
Whether AI becomes a tool for invisible behavioral control (soft totalitarianism) or a support for human autonomy and authentic judgment (the philosophic turn) depends on design choices we make now.

For researchers, ethicists, and technologists concerned with long-term human flourishing, this paper represents a significant reframing of the AI design challenge—from "how do we make AI more persuasive?" to "how do we design AI that augments human judgment while preserving autonomy?"

