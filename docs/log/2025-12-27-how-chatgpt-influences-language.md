---
summary: Analysis of how AI algorithms and platforms subconsciously warp language, trends, and identity. Explores the non-neutral nature of technological tools and their influence on human expression and culture.
event_type: research
sources:
    - https://www.youtube.com/watch?v=ZkXrTHpnQrQ
tags:
    - ai-influence
    - language-evolution
    - algorithms
    - cultural-impact
    - platform-design
    - human-identity
    - llm-effects
---

# Why Are People Starting to Sound Like ChatGPT?

**Speaker:** Adam Aleksic (etymologist)
**Duration:** 5 minutes 12 seconds
**Source:** TED Talks (TEDNext 2025, November 11, 2025)
**Channel:** TED

## Core Argument

Algorithms and AI don't just show us reality—they **warp reality in ways that benefit platforms built to exploit people for profit**. The tools we use to communicate are not neutral; they actively reshape our language, trends, and sense of identity in ways we rarely recognize.

## Key Observations

### The Non-Neutral Tool Thesis
- "These aren't neutral tools"
- Technology platforms have intentional design to influence behavior
- Influence happens **subconsciously**—we don't recognize the shaping

### Language Influence from ChatGPT
- People are beginning to adopt language patterns from AI language models
- ChatGPT's characteristic speech patterns and word choices influence user output
- This creates feedback loops where human-generated content starts resembling AI training data

### Platform Ecosystem Effects

**Example: Spotify's algorithmic influence**
- Data clusters generated by algorithms become new musical genres
- Listeners develop preferences based on what algorithms prioritize
- "New genres" are often just algorithmic constructs, not organic cultural movements

## Broader Implications

### What's Actually Happening
- Algorithms shape what we see and hear (filter bubbles, recommendation systems)
- Platforms profit from engagement, not truth or well-being
- AI tools influence expression without users' awareness
- Feedback loops amplify these effects over time

### Identity and Authenticity
- "Sense of identity" becomes influenced by algorithmic recommendations
- Trends aren't organic grassroots movements—they're algorithmic recommendations
- People adopting AI language patterns without realizing it

### The Profit Motive
- Platforms design systems to exploit psychological vulnerabilities
- Influence is intentional, not accidental
- Users are the product being optimized for engagement, not satisfaction

## Critical Questions Raised

**"How am I being influenced?"**
- Need for constant self-awareness
- Recognizing when our preferences are influenced vs. authentic
- Understanding how tools shape our output

## Relevance to AI-Era Programming and Knowledge Work

This connects directly to several adjacent concerns:

### For Software Development
- Developer tools (IDEs, code editors, LLM assistants) subtly shape coding patterns
- AI-assisted coding influences architecture and design choices
- Teams adopting standardized AI-generated patterns may miss alternative approaches

### For Knowledge Representation
- N4L and knowledge graphs like those in language learning are tools designed by humans
- But even "designed for user freedom" tools contain implicit biases
- Algorithm recommendations in search/navigation shape what gets learned

### For System Design
- Just as platforms optimize for engagement over truth, systems can optimize for metrics that don't serve actual user needs
- "Agent readiness" requires explicit awareness of what systems are optimizing for
- Architecture determines what gets reinforced and what gets ignored

## The Uncomfortable Reality

The speaker emphasizes that technology platforms intentionally warp reality for profit. This isn't a bug; it's the business model:
- Engagement maximization ≠ truth maximization
- User satisfaction ≠ platform profitability
- "AI" tools shaped by recommendation algorithms ≠ neutral assistants

## Open Questions

1. **How does this influence compound over time?** As more people write in ChatGPT-influenced language, what becomes the baseline for "normal" expression?

2. **What are the cultural long-term effects?** If major writing platforms use the same LLM to autocomplete/assist, what happens to linguistic diversity?

3. **Can we design non-exploitative tools?** What would "neutral" AI tools actually look like, and is neutrality even possible?

4. **Where's the feedback loop breaking point?** As AI trains on human-written content that's influenced by AI, where does this converge?

5. **How does this affect specialized domains?** If etymologists, programmers, and mathematicians all use LLM-based tools, do their specialized languages converge toward the LLM's training distribution?

## Related Concepts

This research connects to:
- `2025-12-27-death-and-rebirth-of-programming.md` - Understanding what's actually valuable in the AI era (system design over code)
- `2025-12-27-agent-readiness-framework.md` - How environments and tools shape what's possible
- `2025-12-27-n4l-sst-knowledge-graphs-language-learning.md` - Personal knowledge vs. algorithmically-shaped knowledge
- `2025-12-27-llm-coding-workflow-best-practices.md` - Being intentional about when and how to use LLM assistance

## Practical Implications

**For Anyone Using AI Tools:**
1. **Recognize non-neutrality** - All tools have design biases
2. **Question automation** - When something feels "right" from an AI tool, ask if it's optimal or just what the algorithm learned
3. **Maintain alternatives** - Keep some work separate from algorithmic influence
4. **Track influence** - Notice how tools change your output over time
5. **Consider feedback loops** - Are you training the system that's training you?

## The Fundamental Challenge

As AI systems become more capable and integrated into knowledge work (coding, writing, designing, learning), we need to ask harder questions about:
- What these systems optimize for (engagement? profit? user benefit? truth?)
- How their influence spreads through feedback loops
- Whether we can maintain authentic human expression and thought while relying on tools shaped by other agendas
- How to design systems that amplify human capability without warping human values
